*我也尝试翻译这篇关于介绍Redis分布式锁的文章。对于分布式锁，我以往的认知匮乏，所以难免翻译不到位。但无论咋样，我都会尽力*

## Redis的分布式锁
分布式锁是进程间以排他的方式访问共享资源的一种重要的技术手段。
目前，许多第三方库及博客文章都有介绍如何利用Redis，实现一个DLM(分布式锁管理器)。但在实现方法上，有些简单，有些复杂，而前者还不够完备，无法保证可靠性等。

本文作为一次尝试，提供一种更为通用、规范的算法，叫做**RedLock**，实现上述的DLM(分布式锁)。我们相信它会比仅依赖单台Redis实现的锁，更为完备和安全。希望在这个社区中，该算法能够得到充分的分析，然后将分析结果向我们反馈。依据反馈，我们也努力的让这个算法变得更健壮，亦或者我们可以以此为依据，设计全新的算法。

#### 相关实现
在具体介绍该算法之前，这儿有几个链接，供您参考。
  * [RedLock-cpp](https://github.com/jacket-code/redlock-cpp)(C++ 实现)
  * [Redisson](https://github.com/mrniko/redisson)(java 实现)
  * [Redsync.go](https://github.com/hjr265/redsync.go)(Go 实现)

#### 安全性及可持续性原则
我们给出的方法基于以下三个原则。
因为我们认为，这三个原则是实现一个高性能的分布式锁的最低保证。
1. 安全性原则: 保证排它性。在任意时候，仅有一个客户进程持有锁。
2. 持续性原则A：保证无死锁。即使占有锁的客户端崩溃或者发生网络分区
3. 持续性原则B：保证容错性。只要**多数**的Redis节点正常运行着，客户端便能够获取/释放锁。

#### 仅基于故障切换(failover-based)的方案是不够的
为了弄明白我们想要提高的是什么，先看一下目前大多数基于Redis的分布式锁第三方库的现状。对一个资源用Redis实现加锁，最简单的方法就是在一个实例里，创建一个Key。利用Redis自带的超时特性，使得创建出来的Key一般都有一个有限的存活时间，从而保证了锁能最终能被释放掉(上述列表的第2点)。当客户端需要释放掉这个锁时，删掉创建的Key即可。

单从表面上看来，这一切似乎都是正常的，实则不然。例如，在我们的架构中，存在一个单点故障(a single point of failure)的问题，如果Master节点宕掉了，该怎么办呢？可能有人会说，给Master添加一个slave节点不就行了。这样，当Master节点不可用时，可以切换到Slave。然而很不幸，这种方法依然行不通。因为这么做，违背了上述第1点————排他性，你要知道，Redis的主备复制操作是异步进行的。

在这个方案(model)里，有一个明显的竞争条件：
1. 客户端A在Master节点拿到了锁；
2. 创建的Key在被写入到Slave之前，Master宕掉了；
3. 那么，Slave就晋升为Master了；
4. 客户端B拿到了与A持有的针对相同资源的锁。
这么一来，就违背安全性原则(safety property)。

当然了，在一些特殊的情况下，这种方法也完全可行。例如在宕机期间，多个客户端可以允许持有相同的锁。如果是这样，上述基于复制的解决方案自然没问题。否则的话，建议用本文的方案。

#### 结合单实例的正确实现
在尝试克服掉上述单实例存在的问题之前，先一起看下是否有什么办法，可以修复它。因为在可以容忍竞争条件的应用中，这是一种切实可行的解决方案。而且对于分布式算法，利用单实例的锁策略去实现，是这篇文章的将要描述的算法的基础。

得到这个锁，用下面的命令：

    SET resource_name my_random_value NX PX 30000    
在这里解释一下
  * SET NX 只会在key不存在的时候，给key赋值，PX命令是通知Redis保存这个key值30000ms
  * my_random_value 是一个全局唯一的值，并且必须是全局唯一。这个随机数在释放锁时保证释放锁操作的安全性。
  * PX 30000 后面的参数指代key的存活时间，记作锁过期时间
  * 当资源被锁定，超过了这个时间(这里的话，这个时间是30000ms)，锁将被自动释放。
  * 获得锁的客户端，如果没有在这个时间窗口内完成操作，就有可能由其它客户端获得该锁，引起竞争。
根据这个解释，那么上面的命令的意思就是当且仅当键(key)不存在的时候(NX选项的意思)，设置键(key)的值，并且超时时间定为30000ms(PX选项的作用)。这 个键的值被设为"my\_random\_value"。必须保证该值的全局唯一性，即对所有锁请求的客户端里。基本上使用这个随机值用以保证锁能被以安全的方式释放。利用Lua脚本告知Redis：删除这个键(key)当且仅当该键存在，且与该键对应的值正是所预期的。

脚本命令见下。

    if redis.call("get", KEYS[1]) == ARGV[1] then
        return redis.call("del", KEYS[1])
    else
        return 0;
    end
    
这很重要的，能够避免锁的误删。举例来说，一个客户端可能拿到了锁，因为当中的某个操作而被阻塞，持续的时间超过了指定的有效时长(此时，这个键将过期)后，这个客户端尝试删除这个锁，然而这个锁已被另一个客户端占有。因此，仅使用DEL指令并不安全，很可能导致一个客户端删除了另一个客户端正在持有的锁。上面的Lua脚本示例可以看出，每个客户端对应的锁都用一个随机字符串进行过"签名"，如果要删除锁，只有对应的客户端才可以做到，这就保证了删除锁时候的安全性。

那应当怎样生成这个随机字符串呢？在这里，假设是从 /dev/urandom生成的20个字节大小的随机字符串。当然啦，你可以找到更为高效的方法，确保这个随机字符串的唯一性。例如可以对/dev/urandom使用RC4加密算法，生成一个伪随机流。另一个更简单的方法就是用带毫秒的UNIX时间戳接上客户的ID，这也并不安全，但在大部分环境中，其安全性已经足够了。

键存活时间，也称"锁有效时间"。既是锁自动释放的时间，也是一个客户端的锁在被其它客户端抢占之前，可以执行必要任务的时间。严格来说，在不违背排他性原则的前提下，这个时间仅限于客户端从拿到锁的那一刻起，一个给定的窗口时间。

故现在，我们有一个能够拿到/释放锁的好方法，这对一个单机的、永不宕机的系统来说，能够保证锁的安全性。但如果把这个方法运用到分布式系统当中，就无法保证这一点了。

#### RedLock算法
在这个算法的分布式版本中，我们假定有N个Redis master节点，并且这些节点之间完全独立。没有任何复制或隐式协调操作被应用在节点之间。前面，已经详述了单实例情形下，如何安全地获取/释放锁。因此在Red Lock算法中，同样在单实例的情况下进行获取/释放锁，没有理由不使用这个方法。例如，我们让N = 5，这是一个合理的值，意味着5个Redis master节点在不同的计算机或虚拟机上运行。以此确保在大部分情况下，它们以独立的方式失败(言外之意，就是大多数Master正常运行着)。

要成功获取锁，一个客户端须执行以下操作：
1. 获取当前时间，精确到毫秒；
2. 在这N个节点上，尝试使用相同的键名称和随机值依次获取锁。在这一步执行期间，当客户端在每一个节点(实例)中都设置锁时，使用了一个比*总的锁自动释放时间*(the total lock auto-release time)要小很多的超时时间来取得锁。就比如，如果总的锁自动释放时间被定位10s，那么这个超时时间就被定在大概5-50ms的范围里。这种策略可以防止客户端在一个已经宕掉的Redis节点上，阻塞太长的时间。如果一个节点(实例)不可用，应当尽快尝试下一个节点(实例)。
3. 客户端使用当前时间减去在第1步时取得的时间戳，来计算成功获取到锁需要花费的时间。当且仅当，这个客户端能够在大多数的实例(至少3个实例)中，取得锁，并且所消耗的总时间必须小于锁的有效时间。只有这样，才被认定为客户端成功取得了锁。
4. 锁一旦被获取到，其有效时常为该锁初始的有效时间减去为取得该锁耗费的时间。
5. 如果因为某些原因(要么是未能在至少3(5/2 + 1)个实例中成功获取锁，要么是获取锁耗费的时间大于锁自动释放的时间)，这个客户端未能获取到锁，它都会到每个节点(实例)上，尝试释放锁，哪怕是那些它认为没有成功获取的锁。

#### 这个算法是异步的吗
这个算法是建立在这样的一个假设上的，这个假设就是：虽然不存在跨进程的同步时钟，但仍然可以近似的认为每一个进程中的本地时间都以相同的速度流动。这种假设伴有一定的误差，但这个误差与锁的自动释放时间相比，其实完全可以忽略。这就好比现实中的计算机，每台计算机都有一个本地时钟，虽然时钟间有一定的差异，但这种差异是非常微小的。

现在，需要制定我们算法的互斥规则了: 必须保证持有锁的客户端在锁的有效时间(其计算方法见上面3.)耗尽之时，停止其工作。这个有效时间还得减掉进程间的时钟差，一般只有几毫秒而已。

有关于想要了解更多关于时钟差异(clock drift)的信息，这里有一篇有趣的论文，可以参考一下：[Leases: an efficient fault-tolerant mechanism for distributed file cache consistency](https://dl.acm.org/citation.cfm?id=74870)

#### 失败重试机制
当客户端获取锁失败时，会在一个随机延迟时间后，尝试再获取锁。这个随机延迟时间的设定是为了刻意避开其它客户端也在同一时间重试获取针对相同资源的锁(但这种方式也可能导致一种叫做集群脑裂的情况发生，即导致谁都无法获取锁)。也因此，一个客户端尝试在大多数节点中获取锁的速度越快，出现多个客户端进行脑裂式竞争并且需要重试的时间窗口越小，竞争得到锁的可能性就越低。故理想的情况应当是，这个客户端在同一时刻，采用多路复用的方式去尝试发送**SET**指令给这N个节点。

这里必须强调的是如果客户端没能获取到多数节点(实例)的锁，就一定要尽快释放已经取得的锁。因为没有必要等到键过期，才能重新获取锁(然而，如果发生了网络分区现象，意味着客户端无法与Redis的各个节点通信，这么一来，就得赔上等待键超时这段时间的系统可用性)。

#### 释放锁机制
释放锁相对简单，就是尝试在所有的Redis节点上，进行锁释放操作，而不用管之前对该节点加锁成功与否。

#### 安全性论证
这个算法的安全性如何呢？要搞清楚这个问题，就得理解在不同场景下，使用了该算法后，会发生什么现象。我们假设一个客户端能够获取到多数节点的锁，每一个节点的Key有效时间相同。然而，因为对不同节点的Key设定有效时长时，在不同的时间起点上。因此，键值到期的终点时间也就不同。如果为第一个节点设定过期时长的时间起点在T1时刻(这个时刻，还未与服务器建立连接)，为最后一个节点设定过期时长的时间起点在T2时刻(这个时刻，已经收到了来自服务器端的回应)。那么从T2时刻开始，我们能够确认，首先到期的Key值的存活时间为MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT。之后，其它的键值也将陆续过期。故可以确定，所有的键值，在至少这些MIN_VALIDITY时间里，同时存活着。

一个客户端加锁成功，意味着其它客户端无法抢占这个锁。因为在分布式的情况下，加锁成功意味持有大多数节点的Key。其余的未被持有的Key的数量少于一半，就无法满足另一个客户端要想抢占锁成功，就得持有大多数节点Key这一基本条件。故这样的条件设定符合锁的排他性这一基本件设定。

然而我们要确保的是多个客户端抢占锁时，不能同时成功，这种更为普世的情况，并非只是一两台客户端。

如果一个客户端持有多数节点的锁的时间t1大于了这些锁的最大有效期t2(我们通过set设定的)，那么，被这个客户端持有的这些节点的锁，将会被释放。对于这样的情况，我们无需考虑。而只需要考虑一个客户端持有多数节点的锁并且t1<t2这样的情况。对这种情况，依据上述MIN_VALIDITY已知，没有客户端可以抢占到锁。综合这些分析可知，多客户端同时抢占锁，如果其中有客户端抢占成功，唯一的情况就是t1>t2，key值的有效时间到期，锁自动释放了。

针对这些分析，不知道社区的大神是否可以给出一个形式化的证明，或者找出一个Bug？我们感激不尽。

#### 活性论证
系统的活性(liveness)基于三项主要特征：
1. 自动释放锁(由于Keys过期)：最终Keys可以再次被加锁。
2. 实际上，客户端在遇到下面两种情况时，会主动放弃锁。从而让重新取得锁，又不至于得等到之前的锁得过期才行。
    * 锁本身就没有被成功获取；
    * 锁被获取到了，但自己的事情已经办完了，就没必要再持有锁了。
3. 当一个客户端尝试重新取得锁时，所等待的时间比成功取得多数节点的锁需要的时间大得多话，可以概率性的使抢占资源时，可能出现的脑裂式竞争变得不可能。

但是，在网络分区时，我们得承受TTL时长的可用性损失，一旦有连续的网络分区，需要承受的这种损失将是无限的。每一次，当有客户端得到了锁并在释放锁之前发生了网络分区，这种情况都会发生。基本上来说，如果出现无限连续的网络分区，那么意味着整个系统也将无限期瘫痪掉。

#### 性能，故障恢复以及文件同步(fsync)
许多把Redis用作lock server的用户在以下两个方面上，都有很高的性能要求。对于此种要求，因为涉及到与N个Redis服务器通信以减少延迟，所以在策略选择上肯定采用多路复用(或者也可以采用[模拟多路复用](https://www.irif.fr/~jch/software/polipo/manual/Poor-Mans-Multiplexing.html#Poor-Mans-Multiplexing)的技术，即设置socket为非阻塞模式，发送所有指令，之后再读取所有指令，假定客户端与不同节点的服务端之间往返的时延都相差不大)。
1. 获取/释放锁造成的延迟；
2. 每秒可能执行的acquire/release，这两个操作的次数。
但是如果要针对故障恢复的模型，就得考虑持久性。我们首先看下问题，假设把Redis都配置成非持久化的，一个客户端成功取得了5个节点中的三个锁，但不幸的是这其中有一个节点重启了。意味着这个客户端又有三个锁可以获取，与此同时，另一个节点也可以获得这个锁。这就违反了前面说的锁的互斥原则。

如果我们启动了AOF机制，以上的问题会很好的得到改善，例如我们可以通过发送SHUTDOWN命令然后再重启的方式升级一个服务器。因为Redis的过期时效策略是语义层面上的实现，所以实际上，即使服务器被关闭了，相应键值之前设定的有效时间仍在消耗，也就是说一切仍是正常的。只要正确的关闭，一切都正常。但问题是如果突然断电导致的关闭呢？默认情况下，Redis的AOF永久机制策略采取的是每秒钟向磁盘做一次文件同步操作，那么就有可能在机器重启后，之前设置的Key丢失了。理论上，如果我们要保证锁在面对任何情况下，导致的节点重启时，其都是安全的，就必须在文件同步策略上启用fsync=always。但这反过来会导致系统的性能远不如同级别下，采用其它传统的用于实现分布式锁的算法，实现的系统的性能高。分析到这里，似乎我们的算法并不好，但其实也并不是。基本上，我们的算法安全性是有保证的，只要服务器宕机后，再重启，这个服务器节点不再参与到当前任何处于激活状态的锁，以致于当这个实例重启后，在活跃状态的锁的集合能全部被获取到。

为了保证这一点，我们只需要让一个节点在宕机后变得不可用，这个不可用的时长至少大于当前我们使用的TTL，即这个节点宕掉之后，依然存在的锁的所有键变得无效以及自动释放所需要的时间。

采用延迟重启策略(delayed restarts)，保证锁的安全也是可能的，甚至不需要配置所谓的Redis永久保存机制。但需要注意的是这可能导致可用性缺失。比如如果多数节点宕机，系统将在整个TTL时间里，变得全局不可用(这里的全局意味着这段时间，没有资源可以被加锁)。

#### 扩展锁助其更可靠
如果客户端获取锁的操作可以被分解为更小的步骤，那么默认，锁的有效时间可以用更小的，并且升级这个算法实现一种加锁的扩展机制，是有可能的。如果在计算过程的中间，当锁的有效时间不断趋近一个小的值时，客户端可以发送一个Lua脚本到所有的节点，然后这些节点来增大各自的Key的TTL，当然前提是这些Key必须是存在的并且键的值依然是当锁被获取时，赋予客户端的一个随机值。通过这种方式来扩展加锁机制。

如果客户端能够扩展锁到大多数节点，并且在有效时间内，那么仅考虑重新获取锁应当就可以了(基本上当获取锁时，这个算法之前使用的算法，在策略上非常相似)。

然而严格来说，这依然没有改变原来算法的本质，因此重新取得锁的最大数目应当被限制，否则会违背上述关于锁活性论证中的相应原则。





